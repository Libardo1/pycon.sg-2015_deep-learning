{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Introduction tutorial\n",
    "=====================\n",
    "\n",
    "In this tutorial we will perform handwriting recognition by training a\n",
    "[multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP)\n",
    "on the [MNIST handwritten digit database](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "The Task\n",
    "--------\n",
    "\n",
    "MNIST is a dataset which consists of 70,000 handwritten digits. Each\n",
    "digit is a grayscale image of 28 by 28 pixels. Our task is to classify\n",
    "each of the images into one of the 10 categories representing the\n",
    "numbers from 0 to 9.\n",
    "\n",
    "![Sample MNIST digits](_static/mnist.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model\n",
    "---------\n",
    "\n",
    "We will train a simple MLP with a single hidden layer that uses the\n",
    "[rectifier](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29)\n",
    "activation function. Our output layer will consist of a\n",
    "[softmax](https://en.wikipedia.org/wiki/Softmax_function) function with\n",
    "10 units; one for each class. Mathematically speaking, our model is\n",
    "parametrized by \\\\(\\mathbf{\\theta}\\\\), defined as the weight matrices\n",
    "\\\\(\\mathbf{W}^{(1)}\\\\) and \\\\(\\mathbf{W}^{(2)}\\\\), and bias vectors\n",
    "\\\\(\\mathbf{b}^{(1)}\\\\) and \\\\(\\mathbf{b}^{(2)}\\\\). The rectifier\n",
    "activation function is defined as\n",
    "\n",
    "\\\\[\\mathrm{ReLU}(\\mathbf{x})_i = \\max(0, \\mathbf{x}_i)\\\\]\n",
    "\n",
    "and our softmax output function is defined as\n",
    "\n",
    "\\\\[\\mathrm{softmax}(\\mathbf{x})_i = \\frac{e^{\\mathbf{x}_i}}{\\sum_{j=1}^n e^{\\mathbf{x}_j}}\\\\]\n",
    "\n",
    "Hence, our complete model is\n",
    "\n",
    "\\\\[f(\\mathbf{x}; \\mathbf{\\theta}) = \\mathrm{softmax}(\\mathbf{W}^{(2)}\\mathrm{ReLU}(\\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}) + \\mathbf{b}^{(2)})\\\\]\n",
    "\n",
    "Since the output of a softmax sums to 1, we can interpret it as a\n",
    "categorical probability distribution:\n",
    "\\\\(f(\\mathbf{x})_c = \\hat p(y = c \\mid\n",
    "\\mathbf{x})\\\\), where \\\\(\\mathbf{x}\\\\) is the 784-dimensional (28 Ã— 28)\n",
    "input and \\\\(c \\in \\{0, ..., 9\\}\\\\) one of the 10 classes. We can train\n",
    "the parameters of our model by minimizing the negative log-likelihood\n",
    "i.e. the cross-entropy between our model's output and the target\n",
    "distribution. This means we will minimize the sum of\n",
    "\n",
    "\\\\[l(\\mathbf{f}(\\mathbf{x}), y) = -\\sum_{c=0}^9 \\mathbf{1}_{(y=c)} \\log f(\\mathbf{x})_c = -\\log f(\\mathbf{x})_y\\\\]\n",
    "\n",
    "(where \\\\(\\mathbf{1}\\\\) is the indicator function) over all examples. We\n",
    "use [stochastic gradient\n",
    "descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "(SGD) on mini-batches for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model\n",
    "------------------\n",
    "\n",
    "Blocks uses \"bricks\" to build models. Bricks are **parametrized Theano\n",
    "operations**. You can read more about it in the\n",
    "building with bricks \\<bricks\\> tutorial.\n",
    "\n",
    "Constructing the model with Blocks is very simple. We start by defining\n",
    "the input variable using Theano.\n",
    "\n",
    "> **tip**\n",
    ">\n",
    "> Want to follow along with the Python code? If you are using IPython,\n",
    "> enable the [doctest\n",
    "> mode](http://ipython.org/ipython-doc/dev/interactive/tips.html#run-doctests)\n",
    "> using the special `%doctest_mode` command so that you can copy-paste\n",
    "> the examples below (including the `>>>` prompts) straight into the\n",
    "> IPython interpreter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import tensor \n",
    "x = tensor.matrix('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we picked the name `'features'` for our input. This is\n",
    "important, because the name needs to match the name of the data source\n",
    "we want to train on. MNIST defines two data sources: `'features'` and\n",
    "`'targets'`.\n",
    "\n",
    "For the sake of this tutorial, we will go through building an MLP the\n",
    "long way. For a much quicker way, skip right to the end of the next\n",
    "section. We begin with applying the linear transformations and\n",
    "activations.\n",
    "\n",
    "We start by initializing bricks with certain parameters e.g.\n",
    "`input_dim`. After initialization we can apply our bricks on Theano\n",
    "variables to build the model we want. We'll talk more about bricks in\n",
    "the next tutorial, bricks\\_overview.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import Linear, Rectifier, Softmax\n",
    "input_to_hidden = Linear(name='input_to_hidden', input_dim=784,output_dim=100) \n",
    "h = Rectifier().apply(input_to_hidden.apply(x)) \n",
    "hidden_to_output = Linear(name='hidden_to_output', input_dim=100, output_dim=10) \n",
    "y_hat = Softmax().apply(hidden_to_output.apply(h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function and regularization\n",
    "--------------------------------\n",
    "\n",
    "Now that we have built our model, let's define the cost to minimize. For\n",
    "this, we will need the Theano variable representing the target labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tensor.lmatrix('targets') \n",
    "from blocks.bricks.cost\n",
    "import CategoricalCrossEntropy \n",
    "cost = CategoricalCrossEntropy().apply(y.flatten(), y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the risk of overfitting, we can penalize excessive values of\n",
    "the parameters by adding a \\\\(L2\\\\)-regularization term (also known as\n",
    "*weight decay*) to the objective function:\n",
    "\n",
    "\\\\[l(\\mathbf{f}(\\mathbf{x}), y) = -\\log f(\\mathbf{x})_y + \\lambda_1\\|\\mathbf{W}^{(1)}\\|^2 + \\lambda_2\\|\\mathbf{W}^{(2)}\\|^2\\\\]\n",
    "\n",
    "To get the weights from our model, we will use Blocks' annotation\n",
    "features (read more about them in the cg tutorial).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import WEIGHT \n",
    "from blocks.graph import ComputationGraph \n",
    "from blocks.filter import VariableFilter\n",
    "\n",
    "cg = ComputationGraph(cost) \n",
    "\n",
    "W1, W2 = VariableFilter(roles=[WEIGHT])(cg.variables) \n",
    "\n",
    "cost = cost + 0.005 * (W1 ** 2).sum() + 0.005 * (W2 ** 2).sum() \n",
    "cost.name = 'cost_with_regularization'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **note**\n",
    ">\n",
    "> Note that we explicitly gave our variable a name. We do this so that\n",
    "> when we monitor the performance of our model, the progress monitor\n",
    "> will know what name to report in the logs.\n",
    "\n",
    "Here we set \\\\(\\lambda_1 = \\lambda_2 = 0.005\\\\). And that's it! We now\n",
    "have the final objective function we want to optimize.\n",
    "\n",
    "But creating a simple MLP this way is rather cumbersome. In practice, we\n",
    "would have used the .MLP class instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blocks.bricks import MLP \n",
    "mlp = MLP(activations=[Rectifier(), Softmax()], dims=[784, 100, 10]).apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the parameters\n",
    "---------------------------\n",
    "\n",
    "When we constructed the .Linear bricks to build our model, they\n",
    "automatically allocated Theano shared variables to store their\n",
    "parameters in. All of these parameters were initially set to `NaN`.\n",
    "Before we start training our network, we will want to initialize these\n",
    "parameters by sampling them from a particular probability distribution.\n",
    "Bricks can do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.initialization import IsotropicGaussian,Constant \n",
    "input_to_hidden.weights_init = hidden_to_output.weights_init = IsotropicGaussian(0.01)\n",
    "input_to_hidden.biases_init = hidden_to_output.biases_init = Constant(0) \n",
    "input_to_hidden.initialize()\n",
    "hidden_to_output.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now initialized our weight matrices with entries drawn from a\n",
    "normal distribution with a standard deviation of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1.get_value() \n",
    "# array([[ 0.01624345, -0.00611756, -0.00528172, ..., 0.00043597, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training your model\n",
    "-------------------\n",
    "\n",
    "Besides helping you build models, Blocks also provides the main other\n",
    "features needed to train a model. It has a set of training algorithms\n",
    "(like SGD), an interface to datasets, and a training loop that allows\n",
    "you to monitor and control the training process.\n",
    "\n",
    "We want to train our model on the training set of MNIST. We load the\n",
    "data using the [Fuel](http://fuel.readthedocs.org/en/latest/) framework.\n",
    "Have a look at [this\n",
    "tutorial](https://fuel.readthedocs.org/en/latest/built_in_datasets.html)\n",
    "to get started.\n",
    "\n",
    "After having configured Fuel, you can load the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fuel.datasets import MNIST \n",
    "mnist = MNIST(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets only provide an interface to the data. For actual training, we\n",
    "will need to iterate over the data in minibatches. This is done by\n",
    "initiating a data stream which makes use of a particular iteration\n",
    "scheme. We will use an iteration scheme that iterates over our MNIST\n",
    "examples sequentially in batches of size 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fuel.streams import DataStream \n",
    "from fuel.schemes\n",
    "import SequentialScheme \n",
    "from fuel.transformers import\n",
    "Flatten \n",
    "data\\_stream = Flatten(DataStream.default\\_stream( ...\n",
    "mnist, ... iteration\\_scheme=SequentialScheme(mnist.num\\_examples,\n",
    "batch\\_size=256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm we will use is straightforward SGD with a fixed\n",
    "learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from blocks.algorithms import GradientDescent, Scale \\>\\>\\>\n",
    "algorithm = GradientDescent(cost=cost, params=cg.parameters, ...\n",
    "step\\_rule=Scale(learning\\_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training we will want to monitor the performance of our model on\n",
    "a separate set of examples. Let's create a new data stream for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist\\_test = MNIST(\"test\") \n",
    "data\\_stream\\_test =\n",
    "Flatten(DataStream.default\\_stream( ... mnist\\_test, ...\n",
    "iteration\\_scheme=SequentialScheme( ... mnist\\_test.num\\_examples,\n",
    "batch\\_size=1024)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to monitor our performance on this data stream during training,\n",
    "we need to use one of Blocks' extensions, namely the\n",
    ".DataStreamMonitoring extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from blocks.extensions.monitoring import\n",
    "DataStreamMonitoring \n",
    "monitor = DataStreamMonitoring( ...\n",
    "variables=[cost], data\\_stream=data\\_stream\\_test, prefix=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the .MainLoop to combine all the different bits and\n",
    "pieces. We use two more extensions to make our training stop after a\n",
    "single epoch and to make sure that our progress is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from blocks.main_loop import MainLoop \n",
    "from blocks.extensions import FinishAfter, Printing \n",
    "main_loop = MainLoop(data_stream=data_stream, algorithm=algorithm, ...\n",
    "    extensions=[monitor, FinishAfter(after\\_n\\_epochs=1),\n",
    "    Printing()]) \n",
    "main_loop.run() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "BEFORE FIRST EPOCH \n",
    "-------------------------------------------------------------------------------\n",
    "Training status: epochs\\_done: 0 iterations\\_done: 0 Log records from the\n",
    "iteration 0: test\\_cost\\_with\\_regularization: 2.34244632721\n",
    "\n",
    "-------------------------------------------------------------------------------AFTER\n",
    "ANOTHER\n",
    "EPOCH -------------------------------------------------------------------------------Training\n",
    "status: epochs\\_done: 1 iterations\\_done: 235 Log records from the\n",
    "iteration 235: test\\_cost\\_with\\_regularization: 0.664899230003\n",
    "training\\_finish\\_requested: True \\<BLANKLINE\\>\n",
    "\\<BLANKLINE\\> -------------------------------------------------------------------------------TRAINING\n",
    "HAS BEEN\n",
    "FINISHED: -------------------------------------------------------------------------------Training\n",
    "status: epochs\\_done: 1 iterations\\_done: 235 Log records from the\n",
    "iteration 235: test\\_cost\\_with\\_regularization: 0.664899230003\n",
    "training\\_finish\\_requested: True training\\_finished: True \\<BLANKLINE\\>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
