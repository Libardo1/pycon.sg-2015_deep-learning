{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterwise Double-Stacked LSTM as Author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "from theano import tensor\n",
    "\n",
    "from blocks.bricks import Tanh\n",
    "from blocks.bricks.recurrent import GatedRecurrent\n",
    "from blocks.bricks.sequence_generators import (SequenceGenerator, Readout, SoftmaxEmitter, LookupFeedback)\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "from blocks.algorithms import GradientDescent, Scale\n",
    "from blocks.initialization import Orthogonal, IsotropicGaussian, Constant\n",
    "from blocks.model import Model\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.select import Selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionaries\n",
    "import string\n",
    "\n",
    "all_chars = [ a for a in string.printable]+['<UNK>']\n",
    "code2char = dict(enumerate(all_chars))\n",
    "char2code = {v: k for k, v in code2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rng = numpy.random.RandomState(1)\n",
    "batch_size = 50\n",
    "seq_len = 100\n",
    "\n",
    "num_states=len(char2code)\n",
    "dim = 10\n",
    "feedback_dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from fuel.datasets import Dataset\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ConstantScheme\n",
    "\n",
    "from fuel.datasets import Dataset\n",
    "\n",
    "\n",
    "data_file = '../data/Shakespeare.poetry.txt'\n",
    "\n",
    "#from fuel.datasets import TextFile\n",
    "#dataset = TextFile([data_file], bos_token=None, eos_token=None, level=\"character\", dictionary=char2code)\n",
    "#data_stream = DataStream(dataset, iteration_scheme=ConstantScheme(batch_size))\n",
    "\n",
    "class CharacterTextFile(Dataset):\n",
    "    provides_sources = (\"data\",)\n",
    "\n",
    "    def __init__(self, fname, chunk_len, dictionary, **kwargs):\n",
    "        self.fname = fname\n",
    "        self.chunk_len = chunk_len\n",
    "        self.dictionary = dictionary \n",
    "        super(CharacterTextFile, self).__init__(**kwargs)\n",
    "\n",
    "    def open(self):\n",
    "        return open(self.fname,'r')\n",
    "\n",
    "    def get_data(self, state, request):\n",
    "        assert isinstance(request, int)\n",
    "        x = numpy.zeros((self.chunk_len, request), dtype='int64')\n",
    "        for i in range(request):\n",
    "            txt=state.read(self.chunk_len)\n",
    "            #print(\">%s<\\n\" % (txt,))\n",
    "            x[:, i] = [ self.dictionary[c] for c in txt ]\n",
    "        return (x,)    \n",
    "    \n",
    "    def close(self, state):\n",
    "        close(state)\n",
    "dataset = CharacterTextFile(data_file, chunk_len=seq_len, dictionary=char2code)\n",
    "data_stream = DataStream(CharacterTextFile(data_file, chunk_len=seq_len, dictionary=char2code),\n",
    "                         iteration_scheme=ConstantScheme(batch_size))\n",
    "a=data_stream.get_data(10)\n",
    "#[ code2char[v] for v in [94, 27, 21, 94, 16, 14, 54, 23, 14, 12] ]      # Horizontally\n",
    "#[ code2char[v] for v in [94, 94,95,36,94,47,50,57,40,53,68,54,94,38] ]  # Vertically\n",
    "''.join([ code2char[v] for v in a[0][:,0] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transition = GatedRecurrent(name=\"transition\", dim=dim, activation=Tanh())\n",
    "generator =  SequenceGenerator(\n",
    "                Readout(readout_dim=num_states, source_names=[\"states\"],\n",
    "                        emitter=SoftmaxEmitter(name=\"emitter\"),\n",
    "                        feedback_brick=LookupFeedback(\n",
    "                            num_states, feedback_dim, name='feedback'),\n",
    "                        name=\"readout\"),\n",
    "                transition,\n",
    "                weights_init=IsotropicGaussian(0.01), biases_init=Constant(0),\n",
    "                name=\"generator\")\n",
    "\n",
    "generator.push_initialization_config()\n",
    "transition.weights_init = Orthogonal()\n",
    "generator.initialize()\n",
    "\n",
    "# Give an idea of what's going on.\n",
    "logger.info(\"Parameters:\\n\" + pprint.pformat(\n",
    "    [(key, value.get_value().shape) for key, value in Selector(generator).get_params().items()],\n",
    "    width=120))\n",
    "logger.info(\"Markov chain entropy: {}\".format(MarkovChainDataset.entropy))\n",
    "logger.info(\"Expected min error: {}\".format( -MarkovChainDataset.entropy * seq_len))\n",
    "\n",
    "# Build the cost computation graph.\n",
    "x = tensor.lmatrix('data')\n",
    "cost = aggregation.mean(generator.cost_matrix(x[:, :]).sum(),\n",
    "                        x.shape[1])\n",
    "cost.name = \"sequence_log_likelihood\"\n",
    "\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, params=list(Selector(generator).get_params().values()),\n",
    "    step_rule=Scale(0.001))\n",
    "main_loop = MainLoop(\n",
    "    algorithm=algorithm,\n",
    "    data_stream=DataStream(\n",
    "        MarkovChainDataset(rng, seq_len),\n",
    "        iteration_scheme=ConstantScheme(batch_size)),\n",
    "    model=Model(cost),\n",
    "    extensions=[FinishAfter(after_n_batches=num_batches),\n",
    "                TrainingDataMonitoring([cost], prefix=\"this_step\",\n",
    "                                       after_batch=True),\n",
    "                TrainingDataMonitoring([cost], prefix=\"average\",\n",
    "                                       every_n_batches=100),\n",
    "                Checkpoint(save_path, every_n_batches=500),\n",
    "                Printing(every_n_batches=100)])\n",
    "main_loop.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}